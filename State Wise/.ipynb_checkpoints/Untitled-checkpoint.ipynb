{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d31099-cefe-4664-91b4-3eca175e8fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Excel file: 10%_3MS_NAState_Unfiltered\\Post_Agre_MK.xlsx\n",
      "Results saved to Excel file: 10%_3MS_NAState_Unfiltered\\Post_Agri_LR.xlsx\n",
      "CPU times: total: 48.9 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import winsound\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pymannkendall as mk\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "file_path = 'Offset_Final_resampled_data_quarterly_by_site.csv'             # Replace with the actual path to your CSV file\n",
    "date_end = '2020-12-31'                      # The date prior to which you want to conduct the analysis\n",
    "num_of_years = 20                            # Number of years prior to which the analysis needs to be conducted aka the total timeframe\n",
    "state_filter = False                          # Do you want to filter using a state (True/False)\n",
    "name_state = 'Texas'                      # If True then which state? 'Texas' 'Colorado' 'Oklahoma' 'Kansas' 'Nebraska' 'New Mexico' 'South Dakota'\n",
    "resample_period = '3MS'                      # What is the resample period of your data? # D, B, M, 6M, Q, Y. \n",
    "data_filter = False                           # Do you want to drop sites if they have missing data more than a particular % ereshold? (True/False) \n",
    "threshold = 10                               # What is this threshold in percentage?\n",
    "\n",
    "\"\"\"\n",
    "Frequency strings for time intervals in pandas:\n",
    "D: Calendar daily\n",
    "B: Business daily\n",
    "W: Weekly\n",
    "M: Month end\n",
    "6M: 6 Monthly\n",
    "Q: Quarter end\n",
    "A: Year end\n",
    "H: Hourly\n",
    "T/min: Minutely\n",
    "S: Secondly\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Output Folder Name =  Threshold%_Time Period_ if filtered by state _ if data filtered\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "output_folder = f\"{threshold}%_{resample_period}_{name_state if state_filter else 'NAState'}_{'Filtered' if data_filter else 'Unfiltered'}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Time Frame, Resampling and Unit Selection\n",
    "\n",
    "\"\"\"\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the date column to datetime format with dayfirst=True\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Define the time frame\n",
    "end_date = pd.to_datetime(date_end)  # The prior to which you want to conduct the analysis\n",
    "start_date = end_date - pd.DateOffset(years = num_of_years)  # Number of years prior to which the analysis needs to be conducted\n",
    "\n",
    "# Select the rows within the specified time frame\n",
    "selected_data = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "# Optional: Filter by the \"state\" column to select only rows with \"Nebraska\"\n",
    "filter_state = state_filter  # Set this to False if you don't want to filter by state\n",
    "\n",
    "if filter_state:\n",
    "    state_name = name_state\n",
    "    output_file_name = f'1_data_{state_name}.csv'\n",
    "else:\n",
    "    output_file_name = '1_data.csv'\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Separate the data into four dataframes based on the values' positions within each year\n",
    "dataframes = []\n",
    "\n",
    "for year in range(4):\n",
    "    # Filter data for the specific position within each year\n",
    "    filtered_data = selected_data[selected_data['date'].dt.month.isin([3 * year + 1, 3 * year + 2, 3 * year + 3])]\n",
    "    # Create a new dataframe for the filtered data\n",
    "    dataframes.append(filtered_data)\n",
    "\n",
    "# Now dataframes[0] contains the first value for each year for each site, dataframes[1] contains the second value, and so on\n",
    "\n",
    "#Access the dataframes\n",
    "Q1 = dataframes[0]\n",
    "Q2 = dataframes[1]\n",
    "Q3 = dataframes[2]\n",
    "Q4 = dataframes[3]\n",
    "\n",
    "\n",
    "Q1_output_file_name = 'Q1.csv'\n",
    "Q1_output_file_path = os.path.join(output_folder, Q1_output_file_name)\n",
    "Q1.to_csv(Q1_output_file_path, index=False) \n",
    "\n",
    "Q2_output_file_name = 'Q2.csv'\n",
    "Q2_output_file_path = os.path.join(output_folder, Q2_output_file_name)\n",
    "Q2.to_csv(Q2_output_file_path, index=False)\n",
    "\n",
    "\n",
    "Q3_output_file_name = 'Q3.csv'\n",
    "Q3_output_file_path = os.path.join(output_folder, Q3_output_file_name)\n",
    "Q3.to_csv(Q3_output_file_path, index=False)\n",
    "\n",
    "Q4_output_file_name = 'Q4.csv'\n",
    "Q4_output_file_path = os.path.join(output_folder, Q4_output_file_name)\n",
    "Q4.to_csv(Q4_output_file_path, index=False)\n",
    "\n",
    "\n",
    "# Input whether you want to apply the optional code or not\n",
    "apply_filtering = data_filter  # Change this to True if you want to apply the optional code\n",
    "\n",
    "if apply_filtering:\n",
    "    \n",
    "\n",
    "    # Calculate missing percentages and identify sites to drop\n",
    "    missing_percentage = []\n",
    "    sites_to_drop = []\n",
    "\n",
    "    for quarter_df in dataframes:\n",
    "        missing_percentage.append(\n",
    "            quarter_df.groupby('site')['level'].apply(lambda x: x.isnull().mean() * 100)\n",
    "        )\n",
    "\n",
    "    for quarter_missing_percentage in missing_percentage:\n",
    "        sites_to_drop.append(\n",
    "            quarter_missing_percentage[quarter_missing_percentage > threshold].index.tolist()\n",
    "        )\n",
    "\n",
    "    # Create updated filtered dataframes\n",
    "    updated_dataframes = []\n",
    "\n",
    "    for i, quarter_df in enumerate(dataframes):\n",
    "        updated_df = quarter_df[~quarter_df['site'].isin(sites_to_drop[i])]\n",
    "        updated_dataframes.append(updated_df)\n",
    "\n",
    "    # Save updated dataframes to CSV files\n",
    "    for i, updated_df in enumerate(updated_dataframes):\n",
    "        updated_output_file_name = f'Filtered_Q{i + 1}.csv'\n",
    "        updated_output_file_path = os.path.join(output_folder, updated_output_file_name)\n",
    "        updated_df.to_csv(updated_output_file_path, index=False)\n",
    "\n",
    "    # Merge the four dataframes into one\n",
    "    merged_dataframe = pd.concat(updated_dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Now, 'merged_dataframe' contains all the data from the four dataframes\n",
    "    analysis_data = merged_dataframe.copy()\n",
    "\n",
    "    # Sort the merged_dataframe by 'date' for each 'site'\n",
    "    analysis_data.sort_values(by=['site', 'date'], inplace=True)\n",
    "    analysis_data = analysis_data.dropna()\n",
    "    analysis_data.set_index('date')\n",
    "\n",
    "    \n",
    "else:\n",
    "    analysis_data = selected_data.copy().dropna()\n",
    "    analysis_data.set_index('date')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "Post Aggregation Mann Kendall\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "mk_results = [] # Initialize a list to store results\n",
    "skipped_sites = []  # Initialize a list to store skipped sites\n",
    "\n",
    "# Loop through each unique site in the analysis data\n",
    "for site in analysis_data['site'].unique():\n",
    "    site_data = analysis_data[analysis_data['site'] == site]\n",
    "    for col in ['level']:\n",
    "        # Skip sites with no variability\n",
    "        if site_data[col].nunique() > 1:\n",
    "            # Perform Mann-Kendall test using user-inputted alpha or default value\n",
    "            mk_test = mk.original_test(site_data[col], alpha=0.05)\n",
    "            mk_results.append((site, col, mk_test.trend, mk_test.h,\n",
    "                               mk_test.p, mk_test.z, mk_test.Tau, mk_test.s, mk_test.var_s,\n",
    "                               mk_test.slope, mk_test.intercept, mk_test.slope*80))\n",
    "        else:\n",
    "            skipped_sites.append(site)  # Add the site to the skipped_sites list\n",
    "\n",
    "# Define columns for the Mann-Kendall results\n",
    "mk_columns = ['site', 'data_type', 'trend', 'h', 'p', 'z', 'Tau', 's', 'var_s', 'slope', 'intercept', 'slope m/20y']\n",
    "\n",
    "# Create a DataFrame from the Mann-Kendall results\n",
    "mk_result_df = pd.DataFrame(mk_results, columns=mk_columns)\n",
    "\n",
    "# Calculate the percentage of increasing and decreasing trends\n",
    "trends = mk_result_df[mk_result_df['data_type'] == 'level']['trend']\n",
    "num_increasing = sum(trends == \"increasing\")\n",
    "num_decreasing = sum(trends == \"decreasing\")\n",
    "num_no_trend = sum(trends == \"no trend\")\n",
    "total_trends = len(trends)\n",
    "\n",
    "percentage_increasing = (num_increasing / total_trends) * 100\n",
    "percentage_decreasing = (num_decreasing / total_trends) * 100\n",
    "percentage_no_trend = (num_no_trend / total_trends) * 100\n",
    "\n",
    "# Create DataFrames for the three sheets\n",
    "skipped_sites_df = pd.DataFrame({'Skipped Sites': skipped_sites})\n",
    "trend_percentage_df = pd.DataFrame({\n",
    "    'Number of Increasing Trends': num_increasing,\n",
    "    'Number of Decreasing Trends': num_decreasing,\n",
    "    'Number of No Trends': num_no_trend,\n",
    "    'Percentage of Increasing Trends': percentage_increasing,\n",
    "    'Percentage of Decreasing Trends': percentage_decreasing,\n",
    "    'Percentage of No Trends': percentage_no_trend\n",
    "}, index=[0])\n",
    "\n",
    "\n",
    "# Save results to an Excel file with multiple sheets\n",
    "excel_file_path = os.path.join(output_folder, 'Post_Agre_MK.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "    mk_result_df.to_excel(writer, sheet_name='Mann-Kendall Results', index=False)\n",
    "    skipped_sites_df.to_excel(writer, sheet_name='Skipped Sites', index=False)\n",
    "    trend_percentage_df.to_excel(writer, sheet_name='Trend Percentages', index=False)\n",
    "\n",
    "print(f\"Results saved to Excel file: {excel_file_path}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Post Aggregation Linear Regression with graphs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Perform linear regression for each site and level values\n",
    "\n",
    "linear_regression_results = []\n",
    "skipped_sites = []\n",
    "\n",
    "# Loop through each unique site in the analysis data\n",
    "for site in analysis_data['site'].unique():\n",
    "    site_data = analysis_data[analysis_data['site'] == site]\n",
    "    for col in ['level']:\n",
    "        # Skip sites with no variability\n",
    "        if site_data[col].nunique() > 1:\n",
    "            x = np.arange(len(site_data))  # Assuming x represents time or index\n",
    "            y = site_data[col]\n",
    "            \n",
    "            # Perform linear regression\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "            \n",
    "            # Check if the p-value is less than 0.05 (95% confidence level)\n",
    "            if p_value < 0.05:\n",
    "                trend = \"Increasing\" if slope * 80 > 1 else \"Decreasing\" if slope * 80 < -1 else \"No trend\"  # slope in m per 20 years\n",
    "            else:\n",
    "                trend = \"No trend\"\n",
    "                \n",
    "            linear_regression_results.append((site, col, slope, intercept, trend, slope * 80, p_value))  # Add the slope m/20y and p-value\n",
    "        else:\n",
    "            skipped_sites.append(site)  # Add the site to the skipped_sites list\n",
    "\n",
    "# Define columns for the linear regression results, including the new 'slope m/20y' and 'p-value' columns\n",
    "linear_regression_columns = ['site', 'data_type', 'slope', 'intercept', 'trend', 'slope m/20y', 'p-value']\n",
    "\n",
    "# Create DataFrame for linear regression results\n",
    "linear_regression_df = pd.DataFrame(linear_regression_results, columns=linear_regression_columns)\n",
    "\n",
    "# Calculate the percentage of increasing and decreasing trends for linear regression\n",
    "trends = linear_regression_df[linear_regression_df['data_type'] == 'level']['trend']\n",
    "num_increasing = sum(trends == \"Increasing\")\n",
    "num_decreasing = sum(trends == \"Decreasing\")\n",
    "num_no_trend = sum(trends == \"No trend\")\n",
    "total_trends = len(trends)\n",
    "\n",
    "percentage_increasing = (num_increasing / total_trends) * 100\n",
    "percentage_decreasing = (num_decreasing / total_trends) * 100\n",
    "percentage_no_trend = (num_no_trend / total_trends) * 100\n",
    "\n",
    "# Create DataFrames for the three sheets\n",
    "skipped_sites_df = pd.DataFrame({'Skipped Sites': skipped_sites})\n",
    "trend_percentage_df = pd.DataFrame({\n",
    "    'Number of Increasing Trends': num_increasing,\n",
    "    'Number of Decreasing Trends': num_decreasing,\n",
    "    'Number of No Trends': num_no_trend,\n",
    "    'Percentage of Increasing Trends': percentage_increasing,\n",
    "    'Percentage of Decreasing Trends': percentage_decreasing,\n",
    "    'Percentage of No Trends': percentage_no_trend\n",
    "}, index=[0])\n",
    "\n",
    "# Save results to an Excel file with multiple sheets\n",
    "excel_file_path = os.path.join(output_folder, 'Post_Agri_LR.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:\n",
    "    linear_regression_df.to_excel(writer, sheet_name='Linear Regression Results', index=False)\n",
    "    skipped_sites_df.to_excel(writer, sheet_name='Skipped Sites', index=False)\n",
    "    trend_percentage_df.to_excel(writer, sheet_name='Trend Percentages', index=False)\n",
    "\n",
    "print(f\"Results saved to Excel file: {excel_file_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ada9e-3fe1-417b-adc4-05c12e7f3d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No slope data found for site #152905. Skipping.\n",
      "Skipping site #370552101045701 due to insufficient data.\n",
      "Skipping site #371632100561101 due to insufficient data.\n",
      "Skipping site #373131098531901 due to insufficient data.\n",
      "Skipping site #373134101045301 due to insufficient data.\n",
      "Skipping site #373504101350801 due to insufficient data.\n",
      "Skipping site #373738100510901 due to insufficient data.\n",
      "Skipping site #373940100464501 due to insufficient data.\n",
      "Skipping site #374521100194301 due to insufficient data.\n",
      "Skipping site #374700101032601 due to insufficient data.\n",
      "Skipping site #375006101043801 due to insufficient data.\n",
      "Skipping site #382925101251301 due to insufficient data.\n",
      "Skipping site #392659101174201 due to insufficient data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a folder to save graphs\n",
    "graphs_folder = os.path.join(output_folder, 'graphs')\n",
    "os.makedirs(graphs_folder, exist_ok=True)\n",
    "\n",
    "# Define the offset value (adjust as needed)\n",
    "label_offset = 1  # Adjust the offset as needed\n",
    "\n",
    "# Loop through each unique site\n",
    "for site in analysis_data['site'].unique():\n",
    "    site_data = analysis_data[analysis_data['site'] == site]\n",
    "    \n",
    "    # Check if there's enough data for linear regression\n",
    "    if len(site_data) < 2:\n",
    "        print(f\"Skipping site {site} due to insufficient data.\")\n",
    "        continue\n",
    "    \n",
    "    # Prepare data for linear regression\n",
    "    x = mdates.date2num(site_data['date'])  # Convert datetime to numeric values\n",
    "    y = site_data['level']\n",
    "    \n",
    "    # Perform linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(x.reshape(-1, 1), y)\n",
    "    \n",
    "    # Get the slope (trend) value from the linear_regression_df for the current site\n",
    "    slope_info = linear_regression_df[(linear_regression_df['site'] == site) & (linear_regression_df['data_type'] == 'level')]\n",
    "    \n",
    "    if not slope_info.empty:\n",
    "        slope_m_20y = slope_info['slope m/20y'].values[0]\n",
    "    else:\n",
    "        # Handle the case where there is no data for the current site\n",
    "        print(f\"No slope data found for site {site}. Skipping.\")\n",
    "        continue  # Skip this site and move on to the next one\n",
    "    \n",
    "    # Get the trend information from your linear_regression_results DataFrame\n",
    "    trend_info = linear_regression_df[(linear_regression_df['site'] == site) & (linear_regression_df['data_type'] == 'level')]\n",
    "    \n",
    "    # Extract the trend and p-value\n",
    "    trend = trend_info['trend'].values[0]\n",
    "    p_value = trend_info['p-value'].values[0]\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(23, 10))\n",
    "    \n",
    "    # Plot the time series data in black with a custom legend label including trend, slope, and p-value\n",
    "    label = f'Hydrograph (p-value: {p_value:.3f})'\n",
    "    plt.plot(site_data['date'], y, color='black', label=label, marker='o', markersize=4, linestyle='-')\n",
    "    \n",
    "    # Plot the linear regression line in black with a custom legend label including slope_m_20y\n",
    "    label = f'Regression Line (Slope m/20y: {slope_m_20y:.2f}, Trend: {trend})'\n",
    "    x_regression = mdates.num2date(x)  # Convert numeric values back to datetime\n",
    "    y_regression = model.predict(x.reshape(-1, 1))\n",
    "    plt.plot(x_regression, y_regression, color='black', linestyle='dashed', label=label)\n",
    "    \n",
    "    # Set axes labels and title\n",
    "    ax.set_xlabel('Date', fontsize=18)  # Adjust the fontsize as needed\n",
    "    ax.set_ylabel('Groundwater Level [m]', fontsize=18)  # Adjust the fontsize as needed\n",
    "    ax.set_title(f'Composite Hydrograph for  {site}', fontsize=22)\n",
    "    \n",
    "    # ax.grid()\n",
    "    \n",
    "    # Add legend with increased font size\n",
    "    plt.legend(fontsize=18)\n",
    "    \n",
    "    # Increase the fontsize of the water level numbers on the y-axis\n",
    "    plt.yticks(fontsize=16)  # Adjust the fontsize as needed\n",
    "    \n",
    "    # Set the x-axis locator to display every year\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    \n",
    "    # Rotate and increase the fontsize of the x-axis labels\n",
    "    plt.xticks(rotation=45, fontsize=16)  # Adjust the fontsize as needed\n",
    "    \n",
    "    # Add labels for each data point with customized month values and bbox\n",
    "    for date_val, level_val in zip(site_data['date'], y):\n",
    "        month = date_val.strftime('%b')  # Format to display only the month\n",
    "        if month == 'Mar':\n",
    "            label = 'Q1'\n",
    "        elif month == 'Jun':\n",
    "            label = 'Q2'\n",
    "        elif month == 'Sep':\n",
    "            label = 'Q3'\n",
    "        elif month == 'Dec':\n",
    "            label = 'Q4'\n",
    "        else:\n",
    "            label = month\n",
    "        \n",
    "        # Calculate the label position based on the data point position\n",
    "        label_x = date_val\n",
    "        label_y = level_val + label_offset\n",
    "        \n",
    "        bbox_props = dict(boxstyle='round,pad=0.2', edgecolor='gray', facecolor='white')\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            (label_x, label_y),\n",
    "            xytext=(0, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10,\n",
    "            bbox=bbox_props\n",
    "        )\n",
    "        \n",
    "        # Draw a line from the data point to the label\n",
    "        plt.plot([date_val, label_x], [level_val, label_y], color='gray', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Save the plot as an image without displaying it\n",
    "    plt.savefig(os.path.join(graphs_folder, f'Site_{site}.png'), bbox_inches='tight')\n",
    "    \n",
    "    # Close the plot\n",
    "    plt.close()\n",
    "\n",
    "print(\"Individual site graphs have been saved in the 'graphs' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f817486-92ed-4310-a53c-ac222bf0fd07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre Aggregation Processing\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load and preprocess the data\n",
    "aggregation_data = analysis_data.copy()\n",
    "\n",
    "# Convert 'date' column to DateTime format\n",
    "aggregation_data['date'] = pd.to_datetime(aggregation_data['date'])\n",
    "\n",
    "# Set 'date' as the DataFrame index\n",
    "aggregation_data.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# Calculate the site mean for each site in aggregation_data\n",
    "site_means = aggregation_data.groupby('site')['level'].mean()\n",
    "\n",
    "# Create a new column 'normalised_level' by subtracting the site mean from each level value\n",
    "aggregation_data['normalised_level'] = aggregation_data.apply(lambda row: row['level'] - site_means[row['site']], axis=1)\n",
    "\n",
    "# and calculate the mean of 'normalised_level' for each time step\n",
    "average_normalised_level = aggregation_data['normalised_level'].resample(resample_period).mean()\n",
    "\n",
    "# Optionally, you can reset the index to have 'date' as a regular column\n",
    "average_normalised_level = average_normalised_level.reset_index()\n",
    "average_normalised_level.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "# Save the resampled data to a CSV file\n",
    "average_normalised_level_output_path = os.path.join(output_folder, '3_aggregated_data.csv')\n",
    "average_normalised_level.to_csv(average_normalised_level_output_path, index=True)\n",
    "\n",
    "\n",
    "aggregated_data_df = average_normalised_level.copy().reset_index().dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Pre Aggregated Tests\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Assuming aggregated_data_df is your DataFrame containing the data\n",
    "aggregated_data_df['date'] = pd.to_datetime(aggregated_data_df['date'], format=\"%d-%b-%y\")\n",
    "y_values = aggregated_data_df['normalised_level']\n",
    "x_values = np.linspace(0, 1, len(aggregated_data_df['normalised_level']))\n",
    "\n",
    "# Cleaning NaN\n",
    "idx = np.isfinite(x_values) & np.isfinite(y_values)\n",
    "slope, b = np.polyfit(x_values[idx], y_values[idx], 1)\n",
    "\n",
    "linear_fit = x_values * slope + b\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(23, 10))  # Adjust the figure size (wider)\n",
    "\n",
    "# Drop NaN values\n",
    "aggregated_data_df_cleaned = aggregated_data_df.dropna()\n",
    "\n",
    "# Determine whether the slope is significant\n",
    "p_value_threshold = 0.05\n",
    "slope_significant = p_value < p_value_threshold\n",
    "\n",
    "\n",
    "number_sites = analysis_data['site'].nunique()\n",
    "\n",
    "# Calculate the number of unique sites in each quarter\n",
    "uq1_sites = (len([value for value in Q1['level'] if not pd.isna(value)]) / len(Q1['level'])) * 100\n",
    "uq2_sites = (len([value for value in Q2['level'] if not pd.isna(value)]) / len(Q2['level'])) * 100\n",
    "uq3_sites = (len([value for value in Q3['level'] if not pd.isna(value)]) / len(Q3['level'])) * 100\n",
    "uq4_sites = (len([value for value in Q4['level'] if not pd.isna(value)]) / len(Q4['level'])) * 100\n",
    "\n",
    "# Calculate the number of unique sites in each quarter\n",
    "q1_sites = updated_dataframes[0]['site'].nunique()\n",
    "q2_sites = updated_dataframes[1]['site'].nunique()\n",
    "q3_sites = updated_dataframes[2]['site'].nunique()\n",
    "q4_sites = updated_dataframes[3]['site'].nunique()\n",
    "\n",
    "# Create the legend label with all the information inside the brackets\n",
    "if data_filter:\n",
    "    legend_label = f'Regression Line (Slope m/20y: {slope:.4f}, {\"Significant\" if slope_significant else \"Not Significant\"})\\nNo. of Sites in each Q: Q1 = {q1_sites}, Q2 = {q2_sites}, Q3 = {q3_sites}, Q4 = {q4_sites}'\n",
    "else:\n",
    "    legend_label = f'Regression Line (Slope m/20y: {slope:.4f}, {\"Significant\" if slope_significant else \"Not Significant\"})\\nNo. of Sites: {number_sites}: Q1% = {uq1_sites:.1f}, Q2% = {uq2_sites:.1f}, Q3% = {uq3_sites:.1f}, Q4% = {uq4_sites :.1f}'\n",
    "\n",
    "    \n",
    "# Plot the composite hydrograph (solid)\n",
    "plt.plot(aggregated_data_df['date'], aggregated_data_df['normalised_level'], color='black', label='Composite Hydrograph', marker='o', markersize=6, linestyle='-')\n",
    "\n",
    "# Plot the linear regression line with the updated legend label\n",
    "plt.plot(aggregated_data_df['date'], linear_fit, color='#000000', linestyle='dashed', label=legend_label)\n",
    "\n",
    "# Set axes labels and title with increased font size\n",
    "ax.set_xlabel('Time', fontsize=18)  # Adjust the fontsize as needed\n",
    "ax.set_ylabel('Groundwater Level [m]', fontsize=18)  # Adjust the fontsize as needed\n",
    "ax.set_title('Composite Hydrograph for Selected Wells', fontsize=22)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "# Add legend with increased font size\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "# Increase the fontsize of the water level numbers on the y-axis\n",
    "plt.yticks(fontsize=16)  # Adjust the fontsize as needed\n",
    "\n",
    "# Set the x-axis locator to display every year\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "\n",
    "# Rotate and increase the fontsize of the x-axis labels\n",
    "plt.xticks(rotation=45, fontsize=16)  # Adjust the fontsize as needed\n",
    "\n",
    "# Define label offset\n",
    "label_offset = 0.35\n",
    "\n",
    "# Add data point labels with lines\n",
    "for i, date in enumerate(aggregated_data_df['date']):\n",
    "    month = date.strftime('%b')  # Format to display only the month\n",
    "    if month == 'Mar':\n",
    "        label = 'Q1'\n",
    "    elif month == 'Jun':\n",
    "        label = 'Q2'\n",
    "    elif month == 'Sep':\n",
    "        label = 'Q3'\n",
    "    elif month == 'Dec':\n",
    "        label = 'Q4'\n",
    "    else:\n",
    "        label = month\n",
    "    label_x = date\n",
    "    label_y = aggregated_data_df['normalised_level'].iloc[i] + label_offset\n",
    "    plt.annotate(label, (label_x, label_y), xytext=(0, 0), textcoords='offset points', ha='center', va='bottom', fontsize=10, bbox=dict(boxstyle='round,pad=0.2', edgecolor='gray', facecolor='white'))\n",
    "    \n",
    "    # Draw a line from the data point to the label\n",
    "    plt.plot([date, label_x], [aggregated_data_df['normalised_level'].iloc[i], label_y], color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adjust layout to best fit\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image\n",
    "plot_filename = os.path.join(output_folder, 'time_series_and_regression_with_slope.png')\n",
    "plt.savefig(plot_filename, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x_values[idx], y_values[idx])\n",
    "\n",
    "# Perform Mann-Kendall test\n",
    "mk_result = mk.original_test(y_values)\n",
    "\n",
    "# Define columns for the Mann-Kendall results\n",
    "mk_columns = ['trend', 'h', 'p', 'z', 'Tau', 's', 'var_s', 'sens_slope', 'sens_intercept', 'slope_lr', 'slope m/20y', 'p-value LR', 'Trend Significance']\n",
    "\n",
    "# Determine trend significance based on LR p-value\n",
    "trend_significance = \"Significant\" if p_value < 0.05 else \"Not Significant\"\n",
    "\n",
    "# Create a DataFrame for Mann-Kendall results\n",
    "mk_result_df = pd.DataFrame({\n",
    "    'trend': [trend],\n",
    "    'h': [mk_result.h],\n",
    "    'p': [mk_result.p],\n",
    "    'z': [mk_result.z],\n",
    "    'Tau': [mk_result.Tau],\n",
    "    's': [mk_result.s],\n",
    "    'var_s': [mk_result.var_s],\n",
    "    'sens_slope': [mk_result.slope],\n",
    "    'sens_intercept': [mk_result.intercept],\n",
    "    'slope LR m/20y': [slope],\n",
    "    'slope MK m/20y': [mk_result.slope * 80],\n",
    "    'p-value LR': [p_value],  # Adding the p-value from linear regression\n",
    "    'Trend Significance': [trend_significance],  # Adding the trend significance\n",
    "})\n",
    "\n",
    "# Save the DataFrames to separate Excel files\n",
    "mk_result_excel_path = os.path.join(output_folder, 'Pre_Agre_results.xlsx')\n",
    "mk_result_df.to_excel(mk_result_excel_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "# Convert start_date and end_date to string format\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create the summary message\n",
    "summary = f\"Analysis Date and Time: {current_datetime}\\n\"\n",
    "summary += f\"Start Date: {start_date_str}\\n\"\n",
    "summary += f\"End Date: {end_date_str}\\n\"\n",
    "summary += f\"Number of years: {num_of_years}\\n\"\n",
    "summary += f\"Resampling Period: {resample_period}\\n\"\n",
    "summary += f\"Filter by State: {state_name if filter_state else 'Not Applied'}\\n\"\n",
    "summary += f\"Data Filtering: {'Sites having > 10% mising data removed' if apply_filtering else 'Not Applied'}\\n\"\n",
    "\n",
    "# Save the summary to a text file\n",
    "summary_file_path = os.path.join(output_folder, 'analysis_summary.txt')\n",
    "with open(summary_file_path, 'w') as summary_file:\n",
    "    summary_file.write(summary)\n",
    "\n",
    "print(f\"Analysis summary saved to: {summary_file_path}\")\n",
    "\n",
    "\n",
    "print(\"Code execution finished.\")\n",
    "\n",
    "# Play a bell sound to notify you\n",
    "winsound.Beep(528, 1200)  # Beep at 1000 Hz for 500 milliseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a632d14-6a3b-4196-99b2-5ee08d2cc45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
