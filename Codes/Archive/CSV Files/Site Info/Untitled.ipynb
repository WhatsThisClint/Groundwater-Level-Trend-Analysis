{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a376e8f-c59e-4639-86e4-d9843f85f252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delimiter conversion process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path where your CSV files are located\n",
    "folder_path = 'D:\\IGRAC\\Python\\Site Info'\n",
    "\n",
    "# Create a backup folder to store original files\n",
    "backup_folder = os.path.join(folder_path, 'backup')\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the first line of the file to check the delimiter\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            if ';' in first_line:  # Change delimiter if it's not a comma\n",
    "                new_file_path = os.path.join(backup_folder, filename)\n",
    "                os.rename(file_path, new_file_path)  # Backup original file\n",
    "                \n",
    "                with open(new_file_path, 'w') as new_f:\n",
    "                    for line in f:\n",
    "                        new_line = line.replace(';', ',')\n",
    "                        new_f.write(new_line)\n",
    "                \n",
    "                print(f\"Converted delimiter for '{filename}' and saved backup\")\n",
    "\n",
    "print(\"Delimiter conversion process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70a55cd-076f-435b-a990-3193bccc99c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from 'SITE_INFO Andrews.csv'\n",
      "Read data from 'SITE_INFO Armstrong.csv'\n",
      "Read data from 'SITE_INFO Baca.csv'\n",
      "Read data from 'SITE_INFO Bailey.csv'\n",
      "Read data from 'SITE_INFO Biscoe.csv'\n",
      "Read data from 'SITE_INFO Borden.csv'\n",
      "Read data from 'SITE_INFO Carson.csv'\n",
      "Read data from 'SITE_INFO Castro.csv'\n",
      "Read data from 'SITE_INFO Cheyenne.csv'\n",
      "Read data from 'SITE_INFO Cochran.csv'\n",
      "Read data from 'SITE_INFO Cosby.csv'\n",
      "Read data from 'SITE_INFO Dallam.csv'\n",
      "Read data from 'SITE_INFO Dawson.csv'\n",
      "Read data from 'SITE_INFO Deaf Smith.csv'\n",
      "Read data from 'SITE_INFO Dickens.csv'\n",
      "Read data from 'SITE_INFO Donley.csv'\n",
      "Read data from 'SITE_INFO Floyd.csv'\n",
      "Read data from 'SITE_INFO Gaines.csv'\n",
      "Read data from 'SITE_INFO Garza.csv'\n",
      "Read data from 'SITE_INFO Glasscock.csv'\n",
      "Read data from 'SITE_INFO Gray.csv'\n",
      "Read data from 'SITE_INFO Hale.csv'\n",
      "Read data from 'SITE_INFO Hansford.csv'\n",
      "Read data from 'SITE_INFO Hartley.csv'\n",
      "Read data from 'SITE_INFO Hemphill.csv'\n",
      "Read data from 'SITE_INFO Hockley.csv'\n",
      "Read data from 'SITE_INFO Howard.csv'\n",
      "Read data from 'SITE_INFO Hutchinson.csv'\n",
      "Read data from 'SITE_INFO Kansas 1.csv'\n",
      "Read data from 'SITE_INFO Kansas 2.csv'\n",
      "Read data from 'SITE_INFO Kansas 3.csv'\n",
      "Read data from 'SITE_INFO Kansas 4.csv'\n",
      "Read data from 'SITE_INFO Kiowa.csv'\n",
      "Read data from 'SITE_INFO Kit Carson 1.csv'\n",
      "Read data from 'SITE_INFO Kit Carson 2.csv'\n",
      "Read data from 'SITE_INFO Lamb.csv'\n",
      "Read data from 'SITE_INFO Las Animas.csv'\n",
      "Read data from 'SITE_INFO Lincoln.csv'\n",
      "Read data from 'SITE_INFO Lipscomb.csv'\n",
      "Read data from 'SITE_INFO Logan.csv'\n",
      "Read data from 'SITE_INFO Lubbock.csv'\n",
      "Read data from 'SITE_INFO Lynn.csv'\n",
      "Read data from 'SITE_INFO Martin.csv'\n",
      "Read data from 'SITE_INFO Midland.csv'\n",
      "Read data from 'SITE_INFO Moore.csv'\n",
      "Read data from 'SITE_INFO Motley.csv'\n",
      "Read data from 'SITE_INFO Nabraska.csv'\n",
      "Read data from 'SITE_INFO New Mexico.csv'\n",
      "Read data from 'SITE_INFO Ochiltree.csv'\n",
      "Read data from 'SITE_INFO Oklahoma.csv'\n",
      "Read data from 'SITE_INFO Oldham.csv'\n",
      "Read data from 'SITE_INFO Parmer.csv'\n",
      "Read data from 'SITE_INFO Phillips.csv'\n",
      "Read data from 'SITE_INFO Potter.csv'\n",
      "Read data from 'SITE_INFO Prowers.csv'\n",
      "Read data from 'SITE_INFO Randall.csv'\n",
      "Read data from 'SITE_INFO Roberts.csv'\n",
      "Read data from 'SITE_INFO Sedgwick.csv'\n",
      "Read data from 'SITE_INFO Sherman.csv'\n",
      "Read data from 'SITE_INFO South Dakota.csv'\n",
      "Read data from 'SITE_INFO Swisher.csv'\n",
      "Read data from 'SITE_INFO Terry.csv'\n",
      "Read data from 'SITE_INFO Washington.csv'\n",
      "Read data from 'SITE_INFO Wheeler.csv'\n",
      "Read data from 'SITE_INFO Yoakum.csv'\n",
      "Read data from 'SITE_INFO Yuma 1.csv'\n",
      "Read data from 'SITE_INFO Yuma 2.csv'\n",
      "Merged data saved to merged.csv with the following errors:\n",
      "Error processing file 'merged.csv': No columns to parse from file\n",
      "Error processing file 'site_master.csv': No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder path where your CSV files are located\n",
    "folder_path = 'D:\\IGRAC\\Python\\Site Info'\n",
    "\n",
    "# Create a backup folder to store original files\n",
    "backup_folder = os.path.join(folder_path, 'backup')\n",
    "os.makedirs(backup_folder, exist_ok=True)\n",
    "\n",
    "# List to keep track of errors\n",
    "errors = []\n",
    "\n",
    "# List to store DataFrames read from CSV files\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file with the new delimiter\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "            \n",
    "            print(f\"Read data from '{filename}'\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error processing file '{filename}': {str(e)}\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the merged data to the merged.csv file\n",
    "merged_file_path = os.path.join(folder_path, 'merged.csv')\n",
    "merged_data.to_csv(merged_file_path, index=False)\n",
    "\n",
    "if not errors:\n",
    "    print(\"Merged data saved to merged.csv\")\n",
    "else:\n",
    "    print(\"Merged data saved to merged.csv with the following errors:\")\n",
    "    for error in errors:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b805cc-df21-455f-8f67-f8ade5513ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "1: AgencyCd\n",
      "2: SiteNo\n",
      "3: AgencyNm\n",
      "4: SiteName\n",
      "5: DecLatVa\n",
      "6: DecLongVa\n",
      "7: HorzDatum\n",
      "8: HorzMethod\n",
      "9: HorzAcy\n",
      "10: AltVa\n",
      "11: AltUnits\n",
      "12: AltUnitsNm\n",
      "13: AltDatumCd\n",
      "14: AltMethod\n",
      "15: AltAcy\n",
      "16: WellDepthUnits\n",
      "17: WellDepthUnitsNm\n",
      "18: NatAquiferCd\n",
      "19: NatAqfrDesc\n",
      "20: CountryCd\n",
      "21: CountryNm\n",
      "22: StateCd\n",
      "23: StateNm\n",
      "24: CountyCd\n",
      "25: CountyNm\n",
      "26: LocalAquiferName\n",
      "27: SiteType\n",
      "28: AquiferType\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the columns to drop (comma-separated numbers or ranges, e.g., 1, 3-5, or 'done' to finish):  8,9,11-17,20,21,27\n",
      "Enter the columns to drop (comma-separated numbers or ranges, e.g., 1, 3-5, or 'done' to finish):  done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to cleaned_c_Site_Info_Master.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ask the user for the name of the CSV file\n",
    "csv_filename = \"c_Site_Info_Master.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "try:\n",
    "    data = pd.read_csv(csv_filename)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {csv_filename} not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Display the columns to the user\n",
    "print(\"Columns in the CSV file:\")\n",
    "for idx, column in enumerate(data.columns):\n",
    "    print(f\"{idx + 1}: {column}\")\n",
    "\n",
    "# Ask the user for columns to drop\n",
    "columns_to_drop = []\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"Enter the columns to drop (comma-separated numbers or ranges, e.g., 1, 3-5, or 'done' to finish): \")\n",
    "        if user_input.lower() == 'done':\n",
    "            break\n",
    "        column_selections = user_input.split(\",\")\n",
    "        valid_indices = []\n",
    "        \n",
    "        for selection in column_selections:\n",
    "            if \"-\" in selection:\n",
    "                start, end = map(int, selection.split(\"-\"))\n",
    "                valid_indices.extend(range(start - 1, end))\n",
    "            else:\n",
    "                column_index = int(selection) - 1\n",
    "                if 0 <= column_index < len(data.columns):\n",
    "                    valid_indices.append(column_index)\n",
    "                else:\n",
    "                    print(f\"Invalid input. Column {selection} is not in range.\")\n",
    "            \n",
    "        columns_to_drop.extend([data.columns[idx] for idx in valid_indices])\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter valid column numbers or ranges.\")\n",
    "\n",
    "# Drop the selected columns\n",
    "cleaned_data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "cleaned_filename = f\"cleaned_{csv_filename}\"\n",
    "cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcecc3-04a6-44aa-a3ce-72d428c776e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
